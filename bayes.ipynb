{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Importing important libraries.\n",
    "#Import sys and scipy.io for Linux.\n",
    "#import sys\n",
    "#from scipy.io import arff\n",
    "#import arff for Windows.\n",
    "import arff\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "from numpy import linalg as LA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "########Lodaing of data in Windows.#####################\n",
    "########Training data set (but to delete).############################\n",
    "#raw_data = arff.load(open('lymph_train.arff')) # Loading the training datasets\n",
    "raw_data = arff.load(open('vote_train.arff')) # Loading the training datasets\n",
    "# Creating a list of the features\n",
    "feature_list = [] \n",
    "for x in raw_data['attributes']:\n",
    "    feature_list.append(x[0])\n",
    "training_data = pd.DataFrame(np.array(raw_data['data']), columns = feature_list) # Saving as a Pandas Dataframe\n",
    "training_data = training_data.apply(pd.to_numeric, errors='ignore') # Converting the releavent columns to numeric\n",
    "###################Testing data set (but to delete).###################################\n",
    "#raw_data = arff.load(open('lymph_test.arff')) # Loading the training datasets\n",
    "raw_data = arff.load(open('vote_test.arff')) # Loading the training datasets\n",
    "# Creating a list of the features\n",
    "feature_list = [] \n",
    "for x in raw_data['attributes']:\n",
    "    feature_list.append(x[0])\n",
    "testing_data = pd.DataFrame(np.array(raw_data['data']), columns = feature_list) # Saving as a Pandas Dataframe\n",
    "testing_data = testing_data.apply(pd.to_numeric, errors='ignore') # Converting the releavent columns to numeric\n",
    "#training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Initializing key variables and dictionaries for the code.\n",
    "P_Y = {}\n",
    "P_X_given_Y_positive = {}\n",
    "P_X_given_Y_negative = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Determining the probability of class P(Y).\n",
    "def Prob_Y(data, feature):\n",
    "    classification_frequency = {}\n",
    "    Probability = {}\n",
    "    classification_frequency = data[feature].value_counts().to_dict()\n",
    "    for i in classification_frequency:\n",
    "        Probability[i] = classification_frequency[i]/len(data)\n",
    "    return Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Determining the conditional probability P(X|Y).\n",
    "def Prob_X_condition_Y(data, data_split, feature):\n",
    "    classification_frequency = {}\n",
    "    Probability = {}\n",
    "    classification_frequency = data_split[feature].value_counts().to_dict() #Finding the frequency of sub-features in each column.\n",
    "    feature_data = data[feature].unique() #Finding the unique elements (sub - features) in a column.\n",
    "    for i in feature_data: #Determining the probability for each of the sub - feature in the column given Y (split data).\n",
    "        check = 0\n",
    "        for j in classification_frequency:\n",
    "            if (i == j):\n",
    "                Probability[i] = classification_frequency[j]/len(data_split)\n",
    "                check+=1                \n",
    "        if (check == 0): #If there're no elements corresponding to a sub - feature, then the probability is zero. \n",
    "            Probability[i] = 0.0           \n",
    "    return Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Determining the conditional probability P(X1, X2|Y).\n",
    "def Prob_X1_X2_condition_Y(data_split, feature1, feature2, sub_feature1, sub_feature2):\n",
    "    filtered_data = data_split[(data_split[feature1] == sub_feature1) & (data_split[feature2] == sub_feature2)] #Applying the filter and reducing the data set.\n",
    "    Count_of_filtered_data = len(filtered_data) #Determining the number of data points belonging to (x1, x2).\n",
    "    if (Count_of_filtered_data != 0):\n",
    "        Probability = Count_of_filtered_data/len(data_split) #Determining P(X1, X2|Y) for non-zero data points.             \n",
    "    else:           \n",
    "        Probability = 0.0  #If no data points belonging to (x1, x2), the assigned probability is 0.         \n",
    "    return Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Training of the dataset for naive Bayes algorithm.\n",
    "#Determining the conditional probability for the entire dataset.\n",
    "def Training_naive(data):\n",
    "    prediction_data = data.copy()\n",
    "    P_Y = Prob_Y(data, 'class')\n",
    "    class_labels = data['class'].unique()\n",
    "    data_positive = data[(data['class'] == class_labels[0])].reset_index(drop = True) #Getting the data corresponding to positive label.\n",
    "    data_negative = data[(data['class'] == class_labels[1])].reset_index(drop = True) #Getting the data corresponding to negative label.\n",
    "    #Training the positive values in dataset.\n",
    "    for i in feature_list:\n",
    "        if i == 'class':\n",
    "            break\n",
    "        P_X_given_Y_positive[i] = Prob_X_condition_Y(data, data_positive, i)\n",
    "    #Training the negative values in the dataset.   \n",
    "    for j in feature_list:\n",
    "        if j == 'class':\n",
    "            break\n",
    "        P_X_given_Y_negative[j] = Prob_X_condition_Y(data, data_negative, j)\n",
    "    return P_X_given_Y_positive, P_X_given_Y_negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Prediction for naive Bayes algorithm.\n",
    "def Prediction_naive(data):\n",
    "    Prediction_comparison  = {}\n",
    "    class_labels = data['class'].unique()\n",
    "    sum = 0.0 #Counter variable for number of current predictions.\n",
    "    print('*****************************************************************')\n",
    "    print('***Comparison of actual versus predicted value for Naive Bayes***')\n",
    "    print('*****************************************************************')\n",
    "    for i in range(0, len(data)): #Predicting each row in the testing data set.\n",
    "        accumulated_log_of_positive_probabilities, accumulated_log_of_negative_probabilities = 0, 0 #Initializing the sum of accumulated probability with 0.\n",
    "        row_data = data.iloc[i, :].to_dict() #Reading each row of the data set.\n",
    "        for i in row_data:\n",
    "            for j in P_X_given_Y_positive:        \n",
    "                if (i == j): #Determining the accumulated probability in favor of positive label.\n",
    "                    accumulated_log_of_positive_probabilities+= math.log2 (P_X_given_Y_positive[j][row_data[i]]+1)\n",
    "            for k in P_X_given_Y_negative:\n",
    "                if (i == k): #Determining the accumulated probability in favor of negative label.\n",
    "                    accumulated_log_of_negative_probabilities+= math.log2 (P_X_given_Y_negative[k][row_data[i]]+1)    \n",
    "        if (accumulated_log_of_positive_probabilities > accumulated_log_of_negative_probabilities): #Checking which accumulated probability is greater.\n",
    "            prediction = class_labels[0] #Assigning the predicted label accordingly.\n",
    "        else: \n",
    "            prediction = class_labels[1] #Assigning the predicted label accordingly.\n",
    "        print('Prediced:', prediction, 'Actual: ', row_data['class'])\n",
    "        if (row_data['class'] == prediction): #If prediction is right.\n",
    "            sum+= 1 #Increase the sum by 1.\n",
    "    accuracy = sum/len(data) #Determining the accuracy of the Naive Bayes algorithm.\n",
    "    print('*****************************************************************')\n",
    "    print('Number of data points:', len(data))\n",
    "    print('Number of accurate results: ', sum)\n",
    "    print('Accuracy is: ', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Determining weights for each edges using TAN algorithm.\n",
    "def Weight (X_1, X_2, data):\n",
    "    I_X1_X2 = 0\n",
    "    for y in data['class'].unique(): #Loop over y belonging to labels.\n",
    "        data_split = data[(data['class'] == y)].reset_index(drop = True)\n",
    "        Prob_X1_condition_Y = Prob_X_condition_Y(data, data_split, X_1)\n",
    "        Prob_X2_condition_Y = Prob_X_condition_Y(data, data_split, X_2)\n",
    "        for i in data[X_1].unique(): #Loop over x1 belonging to X1 \n",
    "            for j in data[X_2].unique(): #Loop over x2 belonging to X2 \n",
    "                Z = Prob_X1_X2_condition_Y(data_split, X_1, X_2, i, j) #Determining I(X1, X2| Y) which is the weight for edges for MST. \n",
    "                I_X1_X2+= Prob_Y(data, 'class')[y]*Z*(math.log2((Z+0.0001)/ ((Prob_X1_condition_Y[i]*Prob_X2_condition_Y[j])+0.00001))) #Laplace estimates.\n",
    "    return I_X1_X2 #Returning the weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Determing the weight for the MST.\n",
    "def Weight_collection(data):\n",
    "    from collections import defaultdict #Importing library for 2D dicts.\n",
    "    weight = defaultdict(dict) #Intializing weight.\n",
    "    V = feature_list.copy() #Finding the vertices for the graph.\n",
    "    del V[-1] #Dropping the last column of 'class' from the set of vertices.\n",
    "    for i in V: #Iterating over the set of vertices.\n",
    "        for j in V: #Iterating over the set of vertices.\n",
    "            if (i!=j):\n",
    "                weight[i][j] = Weight(i, j, data)\n",
    "            else:          \n",
    "                weight[i][j] = -1           \n",
    "    return weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Minimum spanning tree (MST).\n",
    "def MST(weight):\n",
    "    V = feature_list.copy() #Initializing V.\n",
    "    del V[-1] #Removing the last column 'class' from V.\n",
    "    V_new = dict() #Initializing empty vertices directory.\n",
    "    E_node = [] #Initializing empty edge list.\n",
    "    E_new = {}  #Initializing empty edge directory.\n",
    "    V_new[V[0]] = V[0] #Assigning the first feature in the dataset as the root of the graph.\n",
    "    selected_origin = \"\"\n",
    "    selected_destination = \"\"\n",
    "    #Starting the maximum spanning tree.\n",
    "    while (set(list(V_new.keys()))!=set(V)):\n",
    "        maximum = 0\n",
    "        update = 0\n",
    "        for i in list(V_new): #Running the loop over all the nodes in V_new.\n",
    "            for j in weight:   #Finding the selected node in weight matrix.\n",
    "                if (i == j):\n",
    "                    for k in weight[j]: #Determining the node to go from the selected node.\n",
    "                        if (weight[j][k] > maximum): #Checking if the weight is maximum to visit that node.\n",
    "                            if k in V_new: #Checking if the node has already been selected or not.\n",
    "                                update+=1 #If the node has already been selected, we don't consider in the present loop.\n",
    "                            if (update == 0): #If the node has not been selected, update the best-so-far node selection.\n",
    "                                maximum = weight[j][k]\n",
    "                                selected_origin = i\n",
    "                                selected_destination = k\n",
    "                        update = 0\n",
    "        V_new[selected_destination] = selected_destination #Adding the node v to V_new.\n",
    "        E_node = [selected_origin, selected_destination] #Adding the edges e to the set of edge E_new.\n",
    "        if selected_origin in E_new.keys():  #Adding the edges in the for of list to create a tree.\n",
    "            E_new[selected_origin].append(selected_destination)  \n",
    "        else:  \n",
    "            E_new[selected_origin] = [selected_destination] \n",
    "    return E_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Determining the conditional probability P(X|X1, Y).\n",
    "def Prob_X_condition_X1_Y(data_split, featureX, featureX1, sub_featureX, sub_featureX1):\n",
    "    data_split = data_split[(data_split[featureX1] == sub_featureX1)].reset_index(drop = True)\n",
    "    filtered_data = data_split[(data_split[featureX] == sub_featureX)] #Applying the filter and reducing the data set.\n",
    "    Count_of_filtered_data = len(filtered_data) #Determining the number of data points belonging to (x1, x2).\n",
    "    if (Count_of_filtered_data != 0):\n",
    "        Probability = Count_of_filtered_data/len(data_split) #Determining P(X1, X2|Y) for non-zero data points.             \n",
    "    else:           \n",
    "        Probability = 0.0  #If no data points belonging to (x1, x2), the assigned probability is 0.         \n",
    "    return Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5126050420168067\n"
     ]
    }
   ],
   "source": [
    "featureX = feature_list[1]\n",
    "featureX1 = feature_list[12]\n",
    "sub_featureX = 'y'\n",
    "sub_featureX1 = 'y'\n",
    "data = training_data\n",
    "class_labels = data['class'].unique()\n",
    "data_split = data[(data['class'] == class_labels[0])].reset_index(drop = True)\n",
    "p = Prob_X_condition_X1_Y(data_split, featureX, featureX1, sub_featureX, sub_featureX1)\n",
    "print (p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'superfund-right-to-sue'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featureX1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['handicapped-infants',\n",
       " 'water-project-cost-sharing',\n",
       " 'adoption-of-the-budget-resolution',\n",
       " 'physician-fee-freeze',\n",
       " 'el-salvador-aid',\n",
       " 'religious-groups-in-schools',\n",
       " 'anti-satellite-test-ban',\n",
       " 'aid-to-nicaraguan-contras',\n",
       " 'mx-missile',\n",
       " 'immigration',\n",
       " 'synfuels-corporation-cutback',\n",
       " 'education-spending',\n",
       " 'superfund-right-to-sue',\n",
       " 'crime',\n",
       " 'duty-free-exports',\n",
       " 'export-administration-act-south-africa',\n",
       " 'class']"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R = feature_list.copy()\n",
    "R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = training_data\n",
    "R = feature_list.copy()\n",
    "from collections import defaultdict #Importing library for 2D dicts.\n",
    "Probability_positive_label = defaultdict(dict) \n",
    "Probability_negative_label = defaultdict(dict)\n",
    "y = 'republican'\n",
    "for i in tree:\n",
    "    data_split = data[(data['class'] == y)].reset_index(drop = True) \n",
    "    Probability_positive_label[tree[i]][i] = Prob_X_condition_X1_Y(data_split, tree[i], i, sub_featureX, sub_featureX1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'aid-to-nicaraguan-contras': ['anti-satellite-test-ban',\n",
       "  'adoption-of-the-budget-resolution'],\n",
       " 'anti-satellite-test-ban': ['export-administration-act-south-africa'],\n",
       " 'el-salvador-aid': ['aid-to-nicaraguan-contras',\n",
       "  'mx-missile',\n",
       "  'physician-fee-freeze'],\n",
       " 'export-administration-act-south-africa': ['immigration'],\n",
       " 'handicapped-infants': ['religious-groups-in-schools'],\n",
       " 'religious-groups-in-schools': ['el-salvador-aid',\n",
       "  'crime',\n",
       "  'superfund-right-to-sue',\n",
       "  'education-spending'],\n",
       " 'superfund-right-to-sue': ['duty-free-exports', 'water-project-cost-sharing'],\n",
       " 'water-project-cost-sharing': ['synfuels-corporation-cutback']}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureX = feature_list[1]\n",
    "featureX1 = feature_list[12]\n",
    "sub_featureX = 'y'\n",
    "sub_featureX1 = 'y'\n",
    "data = training_data\n",
    "class_labels = data['class'].unique()\n",
    "data_split = data[(data['class'] == class_labels[0])].reset_index(drop = True)\n",
    "p = Prob_X_condition_X1_Y(data_split, featureX, featureX1, sub_featureX, sub_featureX1)\n",
    "print (p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calling_model(selection):\n",
    "    if (selection == 'n'):\n",
    "        #Calling the Naive Bayes algorithm.\n",
    "        Training_naive(training_data)\n",
    "        Prediction_naive(testing_data)\n",
    "    if (selection == 't'):\n",
    "        #Calling the Bayesian Network.\n",
    "        weight = Weight_collection(training_data)\n",
    "        tree = MST(weight)\n",
    "        #pd.DataFrame(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "calling_model('t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Reading the Chess-King dataset.\n",
    "raw_data = arff.load(open('chess-KingRookVKingPawn.arff')) # Loading the training datasets\n",
    "# Creating a list of the features\n",
    "feature_list = [] \n",
    "for x in raw_data['attributes']:\n",
    "    feature_list.append(x[0])\n",
    "training_data = pd.DataFrame(np.array(raw_data['data']), columns = feature_list) # Saving as a Pandas Dataframe\n",
    "training_data = training_data.apply(pd.to_numeric, errors='ignore') # Converting the releavent columns to numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stratification(data, n):\n",
    "    #n-fold cross validation.\n",
    "    # Creating a list of the features\n",
    "    feature_list = [] \n",
    "    for x in raw_data['attributes']:\n",
    "        feature_list.append(x[0])    \n",
    "    #Create a empty dataframe to contain the strata of the data.\n",
    "    stratified_data = pd.DataFrame(columns=feature_list)\n",
    "    stratified_data['Fold_of_instance'] = np.nan\n",
    "    #Find number of data-points to go into each strata.\n",
    "    data_count = data['class'].value_counts()\n",
    "    won_data_count = int(data_count['won']/n)\n",
    "    nowin_data_count = int(data_count['nowin']/n)\n",
    "    #Create two partition of the dataset (won and nowin).\n",
    "    won_data = data[data['class'] == 'won'].reset_index(drop = 'True')\n",
    "    nowin_data = data[data['class'] == 'nowin'].reset_index(drop = 'True')\n",
    "    #Appending the split data into a new dataframe with the strata number.\n",
    "    for i in range(0, n-1):\n",
    "        #Appending for won data.\n",
    "        index_for_won_data = np.random.choice(won_data.index.values, won_data_count, replace = False)\n",
    "        won_dataframe  = won_data.iloc[index_for_won_data]\n",
    "        won_data = won_data[~won_data.isin(won_dataframe)].dropna()\n",
    "        won_data = won_data.reset_index(drop = True)\n",
    "        won_dataframe['Fold_of_instance'] = i + 1\n",
    "        stratified_data = stratified_data.append(won_dataframe, ignore_index=True)\n",
    "        #Appending for nowin data.\n",
    "        index_for_nowin_data = random.choice(nowin_data.index.values, nowin_data_count, replace = False)\n",
    "        nowin_dataframe  = nowin_data.iloc[index_for_nowin_data]\n",
    "        nowin_data = nowin_data[~nowin_data.isin(nowin_dataframe)].dropna()\n",
    "        nowin_data = nowin_data.reset_index(drop = True)\n",
    "        nowin_dataframe['Fold_of_instance'] = i + 1\n",
    "        stratified_data = stratified_data.append(nowin_dataframe, ignore_index=True)\n",
    "    #We might have ignored certain values due to roundin off. To consider all the remaining values.\n",
    "    #Appending for won data.\n",
    "    index_for_won_data = np.random.choice(won_data.index.values, len(won_data), replace = False)\n",
    "    won_dataframe  = won_data.iloc[index_for_won_data]\n",
    "    won_data = won_data[~won_data.isin(won_dataframe)].dropna()\n",
    "    won_data = won_data.reset_index(drop = True)\n",
    "    won_dataframe['Fold_of_instance'] = n\n",
    "    stratified_data = stratified_data.append(won_dataframe, ignore_index=True)\n",
    "    #Appending for nowin data.\n",
    "    index_for_nowin_data = random.choice(nowin_data.index.values, len(nowin_data), replace = False)\n",
    "    nowin_dataframe  = nowin_data.iloc[index_for_nowin_data]\n",
    "    nowin_data = nowin_data[~nowin_data.isin(nowin_dataframe)].dropna()\n",
    "    nowin_data = nowin_data.reset_index(drop = True)\n",
    "    nowin_dataframe['Fold_of_instance'] = n\n",
    "    stratified_data = stratified_data.append(nowin_dataframe, ignore_index=True)\n",
    "    #Returning the n - fold dataset.\n",
    "    return stratified_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main output for Chess-King-Rook.\n",
    "# Creating the stratified dataset\n",
    "n = 10\n",
    "stratified_training_data = stratification (training_data, n)\n",
    "for iteration in range(n):\n",
    "    #Training over each stratified dataset.\n",
    "    substratified_training_data = stratified_training_data[stratified_training_data['Fold_of_instance'] != (iteration + 1)]\n",
    "    substratified_training_data = substratified_training_data.reset_index(drop = True)\n",
    "    batch_size = substratified_training_data['class'].value_counts()\n",
    "    training_king = substratified_training_data.iloc[:, 0:len(stratified_training_data.columns)-1]   \n",
    "    Training(training_king)\n",
    "#Working on testing dataset.\n",
    "substratified_testing_data = stratified_training_data[stratified_training_data['Fold_of_instance'] == (iteration + 1)]\n",
    "substratified_testing_data = substratified_testing_data.reset_index(drop = True)\n",
    "testing_king = substratified_testing_data .iloc[:, 0:len(stratified_training_data.columns)-1]\n",
    "#Prediction.\n",
    "Prediction_naive(testing_king)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
